{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "buddhilive_train.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "b2yRsiFMS3L5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Buddhilive Chatbot Training\n",
        "\n",
        "First we are goin to import the necessary libraries and dependencies. We are goint to use Tensorflow and Keras to train this chatbot"
      ]
    },
    {
      "metadata": {
        "id": "1fqLowQ9hKm6",
        "colab_type": "code",
        "outputId": "8ca4b0a8-9de7-4b79-c02a-fe1f9c6c2150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# things we need for NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras import utils\n",
        "from keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6VHudlLSTq9J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Download the dataset\n",
        "\n",
        "The data we need to train the chatbot or the intents are stored in JSON format. We are going to import this data from the GitHub Repository."
      ]
    },
    {
      "metadata": {
        "id": "uWlLSb9COsMa",
        "colab_type": "code",
        "outputId": "a008a1c1-40c9-4256-9bed-fba2f28a7357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Download Dataset\n",
        "!git clone https://github.com/Buddhilive/Buddhilive.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uJQCExU_ULO2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we retrieve data from the downloded dataset.\n",
        "\n",
        "*Note: Data downloded here will be lost when the session is over. You'll need to download the dataset again if you are starting a new session*"
      ]
    },
    {
      "metadata": {
        "id": "mxM-gh4IhKnO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import our chat-bot intents file\n",
        "import json\n",
        "with open('Buddhilive/Datasets/train_intent.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkrlD9HuVAA-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "\n",
        "Now we are going to create a bag of words for the documents and senteces."
      ]
    },
    {
      "metadata": {
        "id": "Ryg-CQx9hKnV",
        "colab_type": "code",
        "outputId": "1d5a88b8-bc0c-44b3-fb4d-14600778a6ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "# loop through each sentence in our intents patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add to our words list\n",
        "        words.extend(w)\n",
        "        # add to documents in our corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# stem and lower each word and remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicates\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XvNus8BwVx68",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we are going to create the training data"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "Lh8z0a-NhKnb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create our training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    #print(classes.index(doc[1]))\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0WReWzZ4hKnh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d_VNQA9UW10X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Start Training\n",
        "\n",
        "Now we are going to start training. This may take some time. Depending on the amount of data, training time may vary."
      ]
    },
    {
      "metadata": {
        "id": "-__GBGVwhKnm",
        "colab_type": "code",
        "outputId": "cb349d65-3701-4519-a87a-39cfd90adac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34272
        }
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(8, input_shape=[len(train_x[0],)]))\n",
        "model.add(Dense(8))\n",
        "model.add(Dense(8))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model.fit(np.array(train_x), np.array(train_y), epochs=1000, batch_size=8)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zbOmhwSwXOeO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After the training is finished, we need to save the trained model"
      ]
    },
    {
      "metadata": {
        "id": "UPZaYtlphKnt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save('Buddhilive_Model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPP_A-kzhKny",
        "colab_type": "code",
        "outputId": "d626f601-19c0-4483-80e9-3ad90524e75b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2-K8TNjEhKn6",
        "colab_type": "code",
        "outputId": "90458f57-773d-405d-d0a0-2793c5eea193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "loss = history.history['loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lBiyau94hKoA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v9surW00hKoF",
        "colab_type": "code",
        "outputId": "d3391db1-27af-4974-c6e9-a2d402986603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "p = bow(\"Hello Bot\", words)\n",
        "print (p)\n",
        "print (classes)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vt8YbrmghKoM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = len(p)\n",
        "f = len(documents)-2\n",
        "a = np.zeros([f, d])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EutYDaxnhKoS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tot = np.vstack((p,a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vPMAl4xhKoX",
        "colab_type": "code",
        "outputId": "15f6a988-5c39-4047-ff0b-cc28a1f0b06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "prediction = model.predict(tot)\n",
        "predicted_index = np.argmax(prediction)\n",
        "predicted_index"
      ],
      "execution_count": 31,
      "outputs": []
    }
  ]
}